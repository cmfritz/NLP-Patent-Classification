{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"deep.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1Bu-I-JhyDSJ5ayYuGL9qQAXBcHJZE7o9","authorship_tag":"ABX9TyNUyW6dU6TYfD5Zrf4u1FMR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"EeuT5CfSceoF","executionInfo":{"status":"ok","timestamp":1630267597607,"user_tz":300,"elapsed":2387,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}}},"source":["# adopted from https://colab.research.google.com/drive/1tGdPsqG-jAmgwRItq1z7oXM5HfYNOuEs?usp=sharing\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import tensorflow as tf\n","from tensorflow.keras import regularizers, optimizers\n","from tensorflow.keras.layers import TextVectorization\n","from tensorflow.keras.layers import Embedding, Dense, Dropout, InputLayer, LSTM, \\\n","Flatten, GlobalMaxPool1D\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.initializers import Constant\n","\n","from nltk.corpus import stopwords\n","from string import punctuation\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i15BzWRScoDD","executionInfo":{"status":"ok","timestamp":1630267598241,"user_tz":300,"elapsed":638,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}},"outputId":"bc8769b5-d328-4d86-b44f-3bb27766b4db"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# import my custom library into colab\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Github/capstone/')\n","import mylibrary as mylib"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CfnwLwk-cyMc","executionInfo":{"status":"ok","timestamp":1630267598242,"user_tz":300,"elapsed":12,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}},"outputId":"5225c146-9693-4d67-d6f6-c1de985e8e4d"},"source":["%cd /content/gdrive/My Drive/Github/capstone"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Github/capstone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1XlmqrZxenDK","executionInfo":{"status":"ok","timestamp":1630267598243,"user_tz":300,"elapsed":9,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}}},"source":["def confusion_plot(y_true, y_pred, labels=None):\n","  conf = confusion_matrix(y_true, y_pred, normalize='true')\n","  ax = sns.heatmap(conf, annot=True, xticklabels=labels, yticklabels=labels,\n","                   cmap=\"Greens\" )\n","  return ax"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIlpTeVThqCF","executionInfo":{"status":"ok","timestamp":1630267598244,"user_tz":300,"elapsed":10,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}}},"source":["def plot_history(history, title=None):\n","    \"\"\"\n","    Given a model history will plot the model training history and \n","    return the last scores for each loss and metric in the  model.\n","    Returns None.\n","    \"\"\"\n","    hist = pd.DataFrame(history.history)\n","    fig = plt.figure(figsize = (10,5))\n","    ax = fig.add_subplot(121)\n","    ax2 = fig.add_subplot(122)\n","    loss = hist.columns[hist.columns.str.endswith('loss')]\n","    accuracy = hist.columns[hist.columns.str.endswith('accuracy')]\n","    hist[loss].plot(title='Loss', ax=ax)\n","    hist[accuracy].plot(title='accuracy', ax=ax2)\n","    plt.title(title)\n","    plt.show()\n","    \n","    for l in loss:\n","        print(f'final {l}: {hist[l].iloc[-1]}')\n","    for r in accuracy:\n","        print(f'final {r}: {hist[r].iloc[-1]}')\n","    plt.show()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSXzBcpsv-IY","executionInfo":{"status":"ok","timestamp":1630267599757,"user_tz":300,"elapsed":1522,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}}},"source":["X = pd.read_pickle('data/X_4class.pkl')\n","y = pd.read_pickle('data/y_4class.pkl')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"NW3ZOtN3_Q4s","executionInfo":{"status":"ok","timestamp":1630267600273,"user_tz":300,"elapsed":520,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}}},"source":["#encode labels for multiclass classification in Keras\n","##### Might not need for 2-class\n","encoder = LabelEncoder()\n","y = encoder.fit_transform(y)\n","y = to_categorical(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n","X_val, X_holdout, y_val, y_holdout = train_test_split(X_test, y_test, test_size = .5, random_state=123)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVrugVki_Vgt","executionInfo":{"status":"ok","timestamp":1630267643223,"user_tz":300,"elapsed":42855,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}}},"source":["#max length of input.  \n","#Will truncate inputs longer and add 0s to the end of sequences that are longer.\n","output_sequence_length = X_train.str.len().max()\n","\n","#define the length of the output sequences.  \n","#Here we are using the length of the longest sentence.  You can make it shorter.\n","vectorizer = TextVectorization(output_sequence_length=output_sequence_length,\n","                               #We want lists of integers as our output.\n","                               #These will be lookup indices in our embedding layer\n","                              #  output_mode='int',\n","                               #We will use the default standardization strategy\n","                               #We could also pass a custom function for custom\n","                               #standardization strategies.\n","                               standardize='lower_and_strip_punctuation')\n","\n","# vectorizer = TextVectorization(standardize='lower_and_strip_punctuation')\n","# #Fit the vectorizer to the training data.\n","# #We need to transform it into a numpy array for this.\n","vectorizer.adapt(X_train.to_numpy())\n","\n","# #We will need the total length of the vocabulary for the embedding layer.\n","vocab_len = vectorizer.vocabulary_size()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"yw-vELtj_ZFL","executionInfo":{"status":"ok","timestamp":1630267643231,"user_tz":300,"elapsed":23,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}}},"source":["def create_LSTM():\n","  model = Sequential()\n","\n","  #Define your input layer for one feature (the whole string) and the dtype.\n","  #If you don't let Keras know to expect a string, it will assume it's looking\n","  #for a float and you'll get an error.\n","  model.add(InputLayer(input_shape=(1,), dtype=tf.string))\n","\n","  #Fitted TextVectorization layer\n","  model.add(vectorizer)\n","\n","  #Untrained Embedding Layer, embedding dimensions of 300\n","  #We'll talk about this layer more a little farther down\n","  model.add(Embedding(vocab_len, 300, input_length=output_sequence_length))\n","  \n","  #The recurrent LSTM layer.  We have it return all of its Y outputs for \n","  #each cycle of each layer.\n","  model.add(LSTM(50, return_sequences=True, \n","                 dropout=0.3, \n","                 kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3)))\n","  #The below below layer returns the highest activation of each LSTM node\n","  #for each input and pass them to the dense layer.\n","  model.add(GlobalMaxPool1D())\n","  \n","  model.add(Dense(50, activation='relu', \n","                  kernel_regularizer = regularizers.l1_l2(l1=1e-4, l2=1e-3)))  \n","  model.add(Dropout(0.3))\n","  \n","  model.add(Dense(50, activation='relu', \n","                  kernel_regularizer = regularizers.l1_l2(l1=1e-4, l2=1e-3)))  \n","  model.add(Dropout(0.3))\n","  \n","  #Add an output layer.  2 nodes for 2 classes and a signoid activation\n","  model.add(Dense(4, activation='softmax'))  ###change to signoid?\n","\n","  optimizer = optimizers.Adam(learning_rate=.01)\n","  model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy']) ###\n","\n","  return model"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdLAO-Pzf2-D","executionInfo":{"status":"ok","timestamp":1630267643232,"user_tz":300,"elapsed":23,"user":{"displayName":"Cat Fritz","photoUrl":"","userId":"13285017268804287022"}}},"source":["use_tpu = True #@param {type:\"boolean\"}\n","\n","if use_tpu:\n","    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n","\n","if 'COLAB_TPU_ADDR' in os.environ:\n","  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n","else:\n","  TF_MASTER=''"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyvHe47I_acx","outputId":"c6746f8e-a1cc-48b9-85a1-134de23e8a09"},"source":["%%time\n","tf.keras.backend.clear_session()\n","\n","self_train = create_LSTM()\n","\n","self_train.summary()\n","\n","self_trained_history = self_train.fit(X_train,\n","                    y_train,\n","                    validation_data = (X_val, y_val),\n","                    epochs = 2,                           ########\n","                    batch_size = 250)                      ########increase\n","\n","self_trained_score = self_train.evaluate(X_val, y_val)\n","\n","print(f'Accuracy on Test Set {self_trained_score[1]}, Loss: {self_trained_score[0]}')\n","plot_history(self_trained_history)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","text_vectorization (TextVect (None, 286270)            0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 286270, 300)       122517300 \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 286270, 50)        70200     \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 50)                0         \n","_________________________________________________________________\n","dense (Dense)                (None, 50)                2550      \n","_________________________________________________________________\n","dropout (Dropout)            (None, 50)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 50)                2550      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 50)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4)                 204       \n","=================================================================\n","Total params: 122,592,804\n","Trainable params: 122,592,804\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"re6tuLGNpN-M"},"source":["self_train.save('/content/gdrive/MyDrive/Github/capstone/data')"],"execution_count":null,"outputs":[]}]}